   0.0 TEL | Telepresence 0.99 launched at Mon Jun 10 08:29:04 2019
   0.0 TEL |   /usr/bin/telepresence --swap-deployment wizfind --expose 8080 --docker-run --rm -p 40000:40000 -p 8080:8080 --security-opt=seccomp:unconfined --name wizdev wizdev
   0.0 TEL | Platform: linux
   0.0 TEL | Python 3.7.3 (default, Mar 26 2019, 21:43:19)
   0.0 TEL | [GCC 8.2.1 20181127]
   0.0 TEL | [1] Running: uname -a
   0.0   1 | Linux hexlyfung 5.1.7-arch1-1-ARCH #1 SMP PREEMPT Tue Jun 4 15:47:45 UTC 2019 x86_64 GNU/Linux
   0.0 TEL | [1] ran in 0.00 secs.
   0.0 TEL | BEGIN SPAN main.py:40(main)
   0.0 TEL | BEGIN SPAN startup.py:74(__init__)
   0.0 TEL | Found kubectl -> /usr/local/bin/kubectl
   0.0 TEL | [2] Capturing: kubectl version --short
   0.3 TEL | [2] captured in 0.26 secs.
   0.3 TEL | [3] Capturing: kubectl config current-context
   0.3 TEL | [3] captured in 0.04 secs.
   0.3 TEL | [4] Capturing: kubectl config view -o json
   0.3 TEL | [4] captured in 0.04 secs.
   0.3 TEL | [5] Capturing: kubectl --context dev get ns dev
   0.6 TEL | [5] captured in 0.25 secs.
   0.6 TEL | Command: kubectl 1.13.3
   0.6 TEL | Context: dev, namespace: dev, version: 1.11.6
   0.6 TEL | Warning: kubectl 1.13.3 may not work correctly with cluster version 1.11.6 due to the version discrepancy. See https://kubernetes.io/docs/setup/version-skew-policy/ for more information.
   0.6 TEL | END SPAN startup.py:74(__init__)    0.6s
   0.6 TEL | Found ssh -> /usr/bin/ssh
   0.6 TEL | [6] Capturing: ssh -V
   0.6 TEL | [6] captured in 0.01 secs.
   0.6 TEL | Found docker -> /usr/bin/docker
   0.6 TEL | Found sshfs -> /usr/bin/sshfs
   0.6 TEL | Found fusermount -> /usr/bin/fusermount
   0.6 TEL | Found sudo -> /usr/bin/sudo
   0.6 TEL | [7] Running: sudo -n echo -n
   0.6 TEL | [7] ran in 0.03 secs.
   0.6 >>> | Volumes are rooted at $TELEPRESENCE_ROOT. See https://telepresence.io/howto/volumes.html for details.
   0.6 TEL | [8] Running: kubectl --context dev --namespace dev get pods telepresence-connectivity-check --ignore-not-found
   1.0 TEL | [8] ran in 0.33 secs.
   1.6 TEL | Scout info: {'latest_version': '0.99', 'application': 'telepresence', 'notices': []}
   1.6 TEL | BEGIN SPAN deployment.py:152(supplant_deployment)
   1.6 >>> | Starting network proxy to cluster by swapping out Deployment wizfind with a proxy
   1.6 TEL | BEGIN SPAN remote.py:78(get_deployment_json)
   1.6 TEL | [9] Capturing: kubectl --context dev --namespace dev get deployment -o json wizfind
   1.8 TEL | [9] captured in 0.25 secs.
   1.8 TEL | END SPAN remote.py:78(get_deployment_json)    0.3s
   1.8 TEL | [10] Running: kubectl --context dev --namespace dev delete deployment wizfind-87d9c75c1f6e4e50b5a66854768b4d23 --ignore-not-found
   2.1 TEL | [10] ran in 0.32 secs.
   2.1 TEL | [11] Running: kubectl --context dev --namespace dev apply -f -
   2.6  11 | deployment.extensions/wizfind-87d9c75c1f6e4e50b5a66854768b4d23 created
   2.6 TEL | [11] ran in 0.48 secs.
   2.6 TEL | [12] Running: kubectl --context dev --namespace dev scale deployment wizfind --replicas=0
   3.0  12 | deployment.extensions/wizfind scaled
   3.0 TEL | [12] ran in 0.37 secs.
   3.0 TEL | END SPAN deployment.py:152(supplant_deployment)    1.4s
   3.0 TEL | BEGIN SPAN remote.py:151(get_remote_info)
   3.0 TEL | BEGIN SPAN remote.py:78(get_deployment_json)
   3.0 TEL | [13] Capturing: kubectl --context dev --namespace dev get deployment -o json --selector=telepresence=87d9c75c1f6e4e50b5a66854768b4d23
   3.3 TEL | [13] captured in 0.33 secs.
   3.3 TEL | END SPAN remote.py:78(get_deployment_json)    0.3s
   3.3 TEL | Searching for Telepresence pod:
   3.3 TEL |   with name wizfind-87d9c75c1f6e4e50b5a66854768b4d23-*
   3.3 TEL |   with labels {'app': 'wizfind', 'release': 'dev', 'telepresence': '87d9c75c1f6e4e50b5a66854768b4d23'}
   3.3 TEL | [14] Capturing: kubectl --context dev --namespace dev get pod -o json --selector=telepresence=87d9c75c1f6e4e50b5a66854768b4d23
   4.6 TEL | [14] captured in 1.29 secs.
   4.6 TEL | Checking wizfind-87d9c75c1f6e4e50b5a66854768b4d23-7b6f64886c-qpxsd
   4.6 TEL | Looks like we've found our pod!
   4.6 TEL | BEGIN SPAN remote.py:113(wait_for_pod)
   4.6 TEL | [15] Capturing: kubectl --context dev --namespace dev get pod wizfind-87d9c75c1f6e4e50b5a66854768b4d23-7b6f64886c-qpxsd -o json
   4.9 TEL | [15] captured in 0.25 secs.
   4.9 TEL | END SPAN remote.py:113(wait_for_pod)    0.2s
   4.9 TEL | END SPAN remote.py:151(get_remote_info)    1.9s
   4.9 TEL | BEGIN SPAN connect.py:36(connect)
   4.9 TEL | [16] Launching kubectl logs: kubectl --context dev --namespace dev logs -f wizfind-87d9c75c1f6e4e50b5a66854768b4d23-7b6f64886c-qpxsd --container wizfind --tail=10
   4.9 TEL | [17] Launching kubectl port-forward: kubectl --context dev --namespace dev port-forward wizfind-87d9c75c1f6e4e50b5a66854768b4d23-7b6f64886c-qpxsd 37403:8022
   4.9 TEL | [18] Running: ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -q -p 37403 telepresence@127.0.0.1 /bin/true
   4.9 TEL | [18] exit 255 in 0.00 secs.
   5.1 TEL | [19] Running: ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -q -p 37403 telepresence@127.0.0.1 /bin/true
   5.1 TEL | [19] exit 255 in 0.02 secs.
   5.2  16 | 2019-06-10T14:29:08+0000 [-] Loading ./forwarder.py...
   5.2  16 | 2019-06-10T14:29:09+0000 [-] /etc/resolv.conf changed, reparsing
   5.2  16 | 2019-06-10T14:29:09+0000 [-] Resolver added ('100.64.0.10', 53) to server list
   5.2  16 | 2019-06-10T14:29:09+0000 [-] SOCKSv5Factory starting on 9050
   5.2  16 | 2019-06-10T14:29:09+0000 [socks.SOCKSv5Factory#info] Starting factory <socks.SOCKSv5Factory object at 0x7f403d990080>
   5.2  16 | 2019-06-10T14:29:09+0000 [-] DNSDatagramProtocol starting on 9053
   5.2  16 | 2019-06-10T14:29:09+0000 [-] Starting protocol <twisted.names.dns.DNSDatagramProtocol object at 0x7f403d990400>
   5.2  16 | 2019-06-10T14:29:09+0000 [-] Loaded.
   5.2  16 | 2019-06-10T14:29:09+0000 [twisted.scripts._twistd_unix.UnixAppLogger#info] twistd 19.2.0 (/usr/bin/python3.6 3.6.5) starting up.
   5.2  16 | 2019-06-10T14:29:09+0000 [twisted.scripts._twistd_unix.UnixAppLogger#info] reactor class: twisted.internet.epollreactor.EPollReactor.
   5.4 TEL | [20] Running: ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -q -p 37403 telepresence@127.0.0.1 /bin/true
   5.4 TEL | [20] exit 255 in 0.01 secs.
   5.4  17 | Forwarding from 127.0.0.1:37403 -> 8022
   5.4  17 | Forwarding from [::1]:37403 -> 8022
   5.7 TEL | [21] Running: ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -q -p 37403 telepresence@127.0.0.1 /bin/true
   5.7  17 | Handling connection for 37403
   6.0 TEL | [21] ran in 0.37 secs.
   6.0 >>> | Forwarding remote port 8080 to local port 8080.
   6.0 >>> | 
   6.0 TEL | Launching Web server for proxy poll
   6.0 TEL | [22] Launching SSH port forward (socks and proxy poll): ssh -N -oServerAliveInterval=1 -oServerAliveCountMax=10 -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -q -p 37403 telepresence@127.0.0.1 -L127.0.0.1:44667:127.0.0.1:9050 -R9055:127.0.0.1:36237
   6.0 TEL | END SPAN connect.py:36(connect)    1.2s
   6.0 TEL | BEGIN SPAN remote_env.py:28(get_remote_env)
   6.0 TEL | [23] Capturing: kubectl --context dev --namespace dev exec wizfind-87d9c75c1f6e4e50b5a66854768b4d23-7b6f64886c-qpxsd --container wizfind -- python3 podinfo.py
   6.1  17 | Handling connection for 37403
   6.8 TEL | [23] captured in 0.76 secs.
   6.8 TEL | END SPAN remote_env.py:28(get_remote_env)    0.8s
   6.8 TEL | BEGIN SPAN mount.py:32(mount_remote_volumes)
   6.8 TEL | [24] Running: sudo sshfs -p 37403 -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -o allow_other telepresence@127.0.0.1:/ /tmp/tel-1e4677dd/fs
   6.8  17 | Handling connection for 37403
   7.3 TEL | [24] ran in 0.46 secs.
   7.3 TEL | END SPAN mount.py:32(mount_remote_volumes)    0.5s
   7.3 TEL | BEGIN SPAN container.py:139(run_docker_command)
   7.3 TEL | [25] Launching Network container: docker run -p=40000:40000 -p=8080:8080 --publish=127.0.0.1:45377:38022/tcp --hostname=wizfind-87d9c75c1f6e4e50b5a66854768b4d23-7b6f64886c-qpxsd --dns=100.64.0.10 --dns-search=dev.svc.cluster.local --dns-search=svc.cluster.local --dns-search=cluster.local --dns-search=us-west-2.compute.internal --dns-opt=ndots:5 --rm --privileged --name=telepresence-1560176952-1486976-14109 datawire/telepresence-local:0.99 proxy '{"cidrs": ["0/0"], "expose_ports": [[8080, 8080]]}'
   7.3 TEL | [26] Running: ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -q -p 45377 root@127.0.0.1 /bin/true
   7.3 TEL | [26] exit 255 in 0.01 secs.
   7.5 TEL | [27] Running: ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -q -p 45377 root@127.0.0.1 /bin/true
   7.8  25 | [INFO  tini (1)] Spawned child process 'python3' with pid '6'
   7.9  25 |    0.0 TEL | Telepresence 0+unknown launched at Mon Jun 10 14:29:12 2019
   7.9  25 |    0.0 TEL |   /usr/bin/entrypoint.py proxy '{"cidrs": ["0/0"], "expose_ports": [[8080, 8080]]}'
   7.9  25 |    0.0 TEL | Platform: linux
   7.9  25 |    0.0 TEL | Python 3.6.5 (default, Aug 22 2018, 14:30:18)
   7.9  25 |    0.0 TEL | [GCC 6.3.0]
   7.9  25 |    0.0 TEL | [1] Running: uname -a
   7.9  25 |    0.0   1 | Linux wizfind-87d9c75c1f6e4e50b5a66854768b4d23-7b6f64886c-qpxsd 5.1.7-arch1-1-ARCH #1 SMP PREEMPT Tue Jun 4 15:47:45 UTC 2019 x86_64 Linux
   7.9  25 |    0.0 TEL | [1] ran in 0.00 secs.
   7.9  25 |    0.0 TEL | [2] Running: /usr/sbin/sshd -e
   7.9  25 |    0.0 TEL | [2] ran in 0.00 secs.
   7.9  25 |    0.0 TEL | [3] Running: ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -q -p 38023 telepresence@127.0.0.1 /bin/true
   7.9  25 |    0.0 TEL | [3] exit 255 in 0.00 secs.
   8.1  25 |    0.3 TEL | [4] Running: ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -q -p 38023 telepresence@127.0.0.1 /bin/true
   8.1  25 |    0.3 TEL | [4] exit 255 in 0.01 secs.
   8.4  25 |    0.5 TEL | [5] Running: ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -q -p 38023 telepresence@127.0.0.1 /bin/true
   8.4  25 |    0.5 TEL | [5] exit 255 in 0.01 secs.
   8.6 TEL | [27] ran in 1.05 secs.
   8.6 TEL | [28] Launching Local SSH port forward: ssh -N -oServerAliveInterval=1 -oServerAliveCountMax=10 -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -q -p 45377 root@127.0.0.1 -R 38023:127.0.0.1:37403
   8.6 TEL | [29] Running: docker run --network=container:telepresence-1560176952-1486976-14109 --rm datawire/telepresence-local:0.99 wait
   8.6  25 |    0.8 TEL | [6] Running: ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -q -p 38023 telepresence@127.0.0.1 /bin/true
   8.6  17 | Handling connection for 37403
   8.9  29 | [INFO  tini (1)] Spawned child process 'python3' with pid '6'
   9.1  25 |    1.2 TEL | [6] ran in 0.43 secs.
   9.1  25 |    1.2 TEL | [7] Capturing: netstat -n
   9.1  25 |    1.2 TEL | [7] captured in 0.02 secs.
   9.1  25 |    1.2 TEL | [8] Launching SSH port forward (exposed ports): ssh -N -oServerAliveInterval=1 -oServerAliveCountMax=10 -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -q -p 38023 telepresence@127.0.0.1 -R '*:8080:127.0.0.1:8080'
   9.1  25 |    1.2 TEL | Everything launched. Waiting to exit...
   9.1  17 | Handling connection for 37403
   9.1  25 |    1.2 TEL | BEGIN SPAN runner.py:669(wait_for_exit)
   9.2  25 | Starting sshuttle proxy.
   9.3  25 | firewall manager: Starting firewall with Python version 3.6.5
   9.3  25 | firewall manager: ready method name nat.
   9.3  25 | IPv6 enabled: False
   9.3  25 | UDP enabled: False
   9.3  25 | DNS enabled: True
   9.3  25 | TCP redirector listening on ('127.0.0.1', 12300).
   9.3  25 | DNS listening on ('127.0.0.1', 12300).
   9.3  25 | Starting client with Python version 3.6.5
   9.3  25 | c : connecting to server...
   9.3  17 | Handling connection for 37403
   9.5  25 | Warning: Permanently added '[127.0.0.1]:38023' (ECDSA) to the list of known hosts.
   9.9  25 | Starting server with Python version 3.6.5
   9.9  25 |  s: latency control setting = True
   9.9  25 |  s: available routes:
   9.9  25 |  s:   2/100.96.2.0/24
   9.9  25 | c : Connected.
   9.9  25 | firewall manager: setting up.
   9.9  25 | >> iptables -t nat -N sshuttle-12300
   9.9  25 | >> iptables -t nat -F sshuttle-12300
   9.9  25 | >> iptables -t nat -I OUTPUT 1 -j sshuttle-12300
   9.9  25 | >> iptables -t nat -I PREROUTING 1 -j sshuttle-12300
   9.9  25 | >> iptables -t nat -A sshuttle-12300 -j RETURN --dest 172.17.0.2/32 -p tcp
   9.9  25 | >> iptables -t nat -A sshuttle-12300 -j RETURN --dest 172.17.0.1/32 -p tcp
   9.9  25 | >> iptables -t nat -A sshuttle-12300 -j RETURN --dest 127.0.0.1/32 -p tcp
   9.9  25 | >> iptables -t nat -A sshuttle-12300 -j REDIRECT --dest 0.0.0.0/0 -p tcp --to-ports 12300 -m ttl ! --ttl 42
   9.9  25 | >> iptables -t nat -A sshuttle-12300 -j REDIRECT --dest 100.64.0.10/32 -p udp --dport 53 --to-ports 12300 -m ttl ! --ttl 42
   9.9  25 | >> iptables -t nat -A sshuttle-12300 -j REDIRECT --dest 224.0.0.252/32 -p udp --dport 5355 --to-ports 12300 -m ttl ! --ttl 42
   9.9  25 | conntrack v1.4.4 (conntrack-tools): 0 flow entries have been deleted.
  11.5  25 | c : DNS request from ('172.17.0.2', 43672) to None: 58 bytes
  11.5  25 | c : DNS request from ('172.17.0.2', 58428) to None: 54 bytes
  12.6  29 | [INFO  tini (1)] Main child exited normally (with status '100')
  12.8 TEL | [29] exit 100 in 4.23 secs.
  12.8 TEL | [30] Capturing: docker run --help
  12.9 TEL | [30] captured in 0.06 secs.
  12.9 TEL | END SPAN container.py:139(run_docker_command)    5.6s
  12.9 >>> | Setup complete. Launching your container.
  12.9 TEL | Everything launched. Waiting to exit...
  12.9 TEL | BEGIN SPAN runner.py:669(wait_for_exit)
  30.7 TEL | [31] Running: sudo -n echo -n
  30.7 TEL | [31] ran in 0.03 secs.
  34.4 TEL | (proxy checking local liveness)
  34.4  16 | 2019-06-10T14:29:39+0000 [Poll#info] Checkpoint
  60.7 TEL | [32] Running: sudo -n echo -n
  60.8 TEL | [32] ran in 0.01 secs.
  64.3 TEL | (proxy checking local liveness)
  64.3  16 | 2019-06-10T14:30:09+0000 [Poll#info] Checkpoint
  90.8 TEL | [33] Running: sudo -n echo -n
  90.8 TEL | [33] ran in 0.01 secs.
  94.3 TEL | (proxy checking local liveness)
  94.3  16 | 2019-06-10T14:30:39+0000 [Poll#info] Checkpoint
 110.8 TEL | END SPAN runner.py:669(wait_for_exit)   97.9s
 110.8 TEL | Main process (docker run --name=telepresence-1560176957-6949015-14109 --network=container:telepresence-1560176952-1486976-14109 -e=TELEPRESENCE_POD -e=TELEPRESENCE_CONTAINER -e=TELEPRESENCE_MOUNTS -e=TELEPRESENCE_CONTAINER_NAMESPACE -e=PAYOUTS_INTEGRATION_SERVICE_PORT -e=PAYOUTS_PERSISTENCE_PORT -e=WIZFIND_PORT -e=WIZFIND_PORT_8080_TCP_PORT -e=DGRAPH_PUBLIC_PORT_5080_TCP_PROTO -e=DGRAPH_PUBLIC_PORT_6080_TCP -e=DGRAPH_PUBLIC_PORT_8080_TCP_PROTO -e=KUBERNETES_SERVICE_HOST -e=PAYOUTS_INTEGRATION_SERVICE_HOST -e=PAYOUTS_INTEGRATION_PORT_5555_TCP_PROTO -e=PAYOUTS_INTEGRATION_PORT_5555_TCP_PORT -e=PAYOUTS_INTEGRATION_PORT_5555_TCP_ADDR -e=PAYOUTS_PERSISTENCE_PORT_5556_TCP_PORT -e=WIZFIND_SERVICE_HOST -e=DGRAPH_PUBLIC_PORT_5080_TCP_ADDR -e=KUBERNETES_PORT_443_TCP_PORT -e=KUBERNETES_PORT_443_TCP_ADDR -e=DGRAPH_PUBLIC_PORT_9080_TCP_ADDR -e=KUBERNETES_PORT -e=KUBERNETES_SERVICE_PORT_HTTPS -e=PAYOUTS_INTEGRATION_PORT -e=WIZFIND_PORT_8080_TCP_PROTO -e=DGRAPH_PUBLIC_PORT -e=DGRAPH_PUBLIC_PORT_5080_TCP -e=DGRAPH_PUBLIC_PORT_5080_TCP_PORT -e=DGRAPH_PUBLIC_PORT_8080_TCP -e=KUBERNETES_SERVICE_PORT -e=DGRAPH_PUBLIC_PORT_9080_TCP -e=DGRAPH_PUBLIC_SERVICE_PORT -e=DGRAPH_PUBLIC_PORT_6080_TCP_PORT -e=DGRAPH_PUBLIC_PORT_8080_TCP_PORT -e=DGRAPH_PUBLIC_PORT_9080_TCP_PORT -e=PAYOUTS_PERSISTENCE_SERVICE_PORT -e=PAYOUTS_PERSISTENCE_SERVICE_HOST -e=WIZFIND_SERVICE_PORT -e=WIZFIND_PORT_8080_TCP_ADDR -e=DGRAPH_PUBLIC_SERVICE_HOST -e=DGRAPH_PUBLIC_SERVICE_PORT_ZERO_HTTP -e=DGRAPH_PUBLIC_SERVICE_PORT_ALPHA_HTTP -e=PAYOUTS_INTEGRATION_PORT_5555_TCP -e=PAYOUTS_PERSISTENCE_PORT_5556_TCP_PROTO -e=PAYOUTS_PERSISTENCE_PORT_5556_TCP_ADDR -e=WIZFIND_PORT_8080_TCP -e=DGRAPH_PUBLIC_SERVICE_PORT_ZERO_GRPC -e=DGRAPH_PUBLIC_PORT_9080_TCP_PROTO -e=PAYOUTS_PERSISTENCE_PORT_5556_TCP -e=KUBERNETES_PORT_443_TCP_PROTO -e=DGRAPH_PUBLIC_SERVICE_PORT_ALPHA_GRPC -e=DGRAPH_PUBLIC_PORT_6080_TCP_PROTO -e=DGRAPH_PUBLIC_PORT_6080_TCP_ADDR -e=DGRAPH_PUBLIC_PORT_8080_TCP_ADDR -e=KUBERNETES_PORT_443_TCP -e=TELEPRESENCE_ROOT -e=TELEPRESENCE_METHOD --volume=/tmp/tel-1e4677dd/fs:/tmp/tel-1e4677dd/fs --init --rm --security-opt=seccomp:unconfined --name wizdev wizdev)
 110.8 TEL |  exited with code 0.
 110.8 >>> | Your process has exited.
 110.8 TEL | EXITING successful session.
 110.8 >>> | Exit cleanup in progress
 110.8 TEL | (Cleanup) Terminate local container
 110.8 TEL | Shutting down containers...
 110.8 TEL | (Cleanup) Kill BG process [28] Local SSH port forward
 110.8  25 | Connection to 127.0.0.1 closed by remote host.
 110.8 TEL | [28] Local SSH port forward: exit 0
 110.8  25 |  102.9   8 | Connection to 127.0.0.1 closed by remote host.
 110.8  25 |  102.9 TEL | [8] SSH port forward (exposed ports): exit 255
 110.8 TEL | (Cleanup) Kill BG process [25] Network container
 110.8 TEL | [34] Running: docker stop --time=1 telepresence-1560176952-1486976-14109
 110.8  25 | >> iptables -t nat -D OUTPUT -j sshuttle-12300
 110.8  25 | >> iptables -t nat -D PREROUTING -j sshuttle-12300
 110.8  25 | >> iptables -t nat -F sshuttle-12300
 110.8  25 | >> iptables -t nat -X sshuttle-12300
 110.8  25 | firewall manager: Error trying to undo /etc/hosts changes.
 110.8  25 | firewall manager: ---> Traceback (most recent call last):
 110.8  25 | firewall manager: --->   File "/usr/lib/python3.6/site-packages/sshuttle/firewall.py", line 274, in main
 110.8  25 | firewall manager: --->     restore_etc_hosts(port_v6 or port_v4)
 110.8  25 | firewall manager: --->   File "/usr/lib/python3.6/site-packages/sshuttle/firewall.py", line 50, in restore_etc_hosts
 110.8  25 | firewall manager: --->     rewrite_etc_hosts({}, port)
 110.8  25 | firewall manager: --->   File "/usr/lib/python3.6/site-packages/sshuttle/firewall.py", line 29, in rewrite_etc_hosts
 110.8  25 | firewall manager: --->     os.link(HOSTSFILE, BAKFILE)
 110.8  25 | firewall manager: ---> OSError: [Errno 18] Cross-device link: '/etc/hosts' -> '/etc/hosts.sbak'
 110.8  25 | c : fatal: server died with error code 255
 110.8  25 | [INFO  tini (1)] Main child exited with signal (with signal 'Terminated')
 111.1  34 | telepresence-1560176952-1486976-14109
 111.1 TEL | [34] ran in 0.29 secs.
 111.1 TEL | (Cleanup) Unmount remote filesystem
 111.1 TEL | [35] Running: sudo fusermount -z -u /tmp/tel-1e4677dd/fs
 111.1 TEL | [25] Network container: exit 143
 111.1 TEL | [35] ran in 0.02 secs.
 111.1 TEL | (Cleanup) Kill BG process [22] SSH port forward (socks and proxy poll)
 111.1 TEL | [22] SSH port forward (socks and proxy poll): exit 0
 111.1 TEL | (Cleanup) Kill Web server for proxy poll
 111.3 TEL | (Cleanup) Kill BG process [17] kubectl port-forward
 111.3 TEL | [17] kubectl port-forward: exit -15
 111.3 TEL | (Cleanup) Kill BG process [16] kubectl logs
 111.3 TEL | [16] kubectl logs: exit -15
 111.3 TEL | Background process (kubectl logs) exited with return code -15. Command was:
 111.3 TEL |   kubectl --context dev --namespace dev logs -f wizfind-87d9c75c1f6e4e50b5a66854768b4d23-7b6f64886c-qpxsd --container wizfind --tail=10
 111.3 TEL | 
 111.3 TEL | Recent output was:
 111.3 TEL |   2019-06-10T14:29:09+0000 [-] SOCKSv5Factory starting on 9050
 111.3 TEL |   2019-06-10T14:29:09+0000 [socks.SOCKSv5Factory#info] Starting factory <socks.SOCKSv5Factory object at 0x7f403d990080>
 111.3 TEL |   2019-06-10T14:29:09+0000 [-] DNSDatagramProtocol starting on 9053
 111.3 TEL |   2019-06-10T14:29:09+0000 [-] Starting protocol <twisted.names.dns.DNSDatagramProtocol object at 0x7f403d990400>
 111.3 TEL |   2019-06-10T14:29:09+0000 [-] Loaded.
 111.3 TEL |   2019-06-10T14:29:09+0000 [twisted.scripts._twistd_unix.UnixAppLogger#info] twistd 19.2.0 (/usr/bin/python3.6 3.6.5) starting up.
 111.3 TEL |   2019-06-10T14:29:09+0000 [twisted.scripts._twistd_unix.UnixAppLogger#info] reactor class: twisted.internet.epollreactor.EPollReactor.
 111.3 TEL |   2019-06-10T14:29:39+0000 [Poll#info] Checkpoint
 111.3 TEL |   2019-06-10T14:30:09+0000 [Poll#info] Checkpoint
 111.3 TEL |   2019-06-10T14:30:39+0000 [Poll#info] Checkpoint
 111.3 TEL | (Cleanup) Re-scale original deployment
 111.3 TEL | [36] Running: kubectl --context dev --namespace dev scale deployment wizfind --replicas=1
 111.8  36 | deployment.extensions/wizfind scaled
 111.8 TEL | [36] ran in 0.43 secs.
 111.8 TEL | (Cleanup) Delete new deployment
 111.8 >>> | Swapping Deployment wizfind back to its original state
 111.8 TEL | [37] Running: kubectl --context dev --namespace dev delete deployment wizfind-87d9c75c1f6e4e50b5a66854768b4d23
 112.1  37 | deployment.extensions "wizfind-87d9c75c1f6e4e50b5a66854768b4d23" deleted
 112.1 TEL | [37] ran in 0.35 secs.
 112.1 TEL | (Cleanup) Kill sudo privileges holder
 112.1 TEL | (Cleanup) Stop time tracking
 112.1 TEL | END SPAN main.py:40(main)  112.1s
 112.1 TEL | SPAN SUMMARY:
 112.1 TEL |  112.1s main.py:40(main)
 112.1 TEL |    0.6s   startup.py:74(__init__)
 112.1 TEL |    0.3s     2 kubectl version --short
 112.1 TEL |    0.0s     3 kubectl config current-context
 112.1 TEL |    0.0s     4 kubectl config view -o json
 112.1 TEL |    0.3s     5 kubectl --context dev get ns dev
 112.1 TEL |    0.0s   6 ssh -V
 112.1 TEL |    0.0s   7 sudo -n echo -n
 112.1 TEL |    0.3s   8 kubectl --context dev --namespace dev get pods telepresence-connectivity-check
 112.1 TEL |    1.4s   deployment.py:152(supplant_deployment)
 112.1 TEL |    0.3s     remote.py:78(get_deployment_json)
 112.1 TEL |    0.2s       9 kubectl --context dev --namespace dev get deployment -o json wizfind
 112.1 TEL |    0.3s     10 kubectl --context dev --namespace dev delete deployment wizfind-87d9c75c1f6e4
 112.1 TEL |    0.5s     11 kubectl --context dev --namespace dev apply -f -
 112.1 TEL |    0.4s     12 kubectl --context dev --namespace dev scale deployment wizfind --replicas=0
 112.1 TEL |    1.9s   remote.py:151(get_remote_info)
 112.1 TEL |    0.3s     remote.py:78(get_deployment_json)
 112.1 TEL |    0.3s       13 kubectl --context dev --namespace dev get deployment -o json --selector=telep
 112.1 TEL |    1.3s     14 kubectl --context dev --namespace dev get pod -o json --selector=telepresence
 112.1 TEL |    0.2s     remote.py:113(wait_for_pod)
 112.1 TEL |    0.2s       15 kubectl --context dev --namespace dev get pod wizfind-87d9c75c1f6e4e50b5a6685
 112.1 TEL |    1.2s   connect.py:36(connect)
 112.1 TEL |    0.0s     18 ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -q
 112.1 TEL |    0.0s     19 ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -q
 112.1 TEL |    0.0s     20 ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -q
 112.1 TEL |    0.4s     21 ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -q
 112.1 TEL |    0.8s   remote_env.py:28(get_remote_env)
 112.1 TEL |    0.8s     23 kubectl --context dev --namespace dev exec wizfind-87d9c75c1f6e4e50b5a6685476
 112.1 TEL |    0.5s   mount.py:32(mount_remote_volumes)
 112.1 TEL |    0.5s     24 sudo sshfs -p 37403 -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsF
 112.1 TEL |    5.6s   container.py:139(run_docker_command)
 112.1 TEL |    0.0s     26 ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -q
 112.1 TEL |    1.1s     27 ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -q
 112.1 TEL |    4.2s     29 docker run --network=container:telepresence-1560176952-1486976-14109 --rm dat
 112.1 TEL |    0.1s     30 docker run --help
 112.1 TEL |   97.9s   runner.py:669(wait_for_exit)
 112.1 TEL |    0.0s     31 sudo -n echo -n
 112.1 TEL |    0.0s     32 sudo -n echo -n
 112.1 TEL |    0.0s     33 sudo -n echo -n
 112.1 TEL |    0.3s   34 docker stop --time=1 telepresence-1560176952-1486976-14109
 112.1 TEL |    0.0s   35 sudo fusermount -z -u /tmp/tel-1e4677dd/fs
 112.1 TEL |    0.4s   36 kubectl --context dev --namespace dev scale deployment wizfind --replicas=1
 112.1 TEL |    0.4s   37 kubectl --context dev --namespace dev delete deployment wizfind-87d9c75c1f6e4
 112.1 TEL | (Cleanup) Remove temporary directory
 112.1 TEL | (Cleanup) Save caches
 112.8 TEL | (sudo privileges holder thread exiting)
